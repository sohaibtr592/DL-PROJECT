import json
import joblib
import numpy as np
import pandas as pd
import streamlit as st

import torch
import torch.nn as nn


# =====================================================
# 1) D√©finition du mod√®le LSTM (m√™me archi que notebook)
# =====================================================

class LSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        # x : (batch, seq_len, input_dim)
        out, (hn, cn) = self.lstm(x)
        last_hidden = hn[-1]           # (batch, hidden_dim)
        logits = self.fc(last_hidden)  # (batch, num_classes)
        return logits


# =====================================================
# 2) Chargement des donn√©es / mod√®les (avec cache)
# =====================================================

@st.cache_data
def load_data_and_features():
    """Charge le dataset pr√©par√© et la liste des features."""
    df = pd.read_csv("df_ready_for_app.csv")
    with open("feature_cols.json", "r") as f:
        feature_cols = json.load(f)
    return df, feature_cols


@st.cache_resource
def load_rf_model():
    """Charge le mod√®le RandomForest entra√Æn√© (Phase 1)."""
    return joblib.load("random_forest.pkl")


@st.cache_resource
def load_lstm_model(input_dim, num_classes, hidden_dim=64, num_layers=1):
    """Charge le mod√®le LSTM (poids) + retourne le device."""
    device = torch.device("cpu")   # Streamlit Cloud = CPU
    model = LSTMClassifier(input_dim, hidden_dim, num_layers, num_classes).to(device)

    state_dict = torch.load("lstm_rmsprop.pt", map_location=device)
    model.load_state_dict(state_dict)
    model.eval()

    return model, device


@st.cache_resource
def load_scaler_lstm():
    """Charge le scaler utilis√© pendant l'entra√Ænement du LSTM."""
    scaler = joblib.load("scaler_lstm.pkl")
    return scaler


@st.cache_data
def prepare_sequential_data(df, feature_cols, seq_len=10):
    """
    Trie le dataframe dans l'ordre temporel, pr√©pare X_all (features)
    et pr√©-calcul X_all_scaled pour le LSTM.
    """
    df_seq = df.sort_values("timestamp").reset_index(drop=True)
    X_all = df_seq[feature_cols].values.astype("float32")

    scaler = load_scaler_lstm()
    X_all_scaled = scaler.transform(X_all)

    return df_seq, X_all, X_all_scaled


# =====================================================
# 3) Fonctions utilitaires
# =====================================================

def create_seq_from_index(X_all_scaled, idx_last, seq_len=10):
    """
    Construit une s√©quence (seq_len, n_features) termin√©e √† idx_last.
    idx_last doit √™tre >= seq_len-1.
    """
    start = idx_last - seq_len + 1
    X_seq = X_all_scaled[start:idx_last+1, :]
    return X_seq


def describe_time_window(series_vals):
    """
    G√©n√®re un court r√©sum√© textuel d'une s√©rie temporelle :
    min, max, moyenne, tendance, volatilit√©.
    """
    val_min = float(np.min(series_vals))
    val_max = float(np.max(series_vals))
    val_mean = float(np.mean(series_vals))
    start_val = float(series_vals[0])
    end_val = float(series_vals[-1])
    amplitude = max(val_max - val_min, 1e-8)
    delta = end_val - start_val

    # tendance globale
    if delta > 0.15 * amplitude:
        trend = "globalement **croissante**"
    elif delta < -0.15 * amplitude:
        trend = "globalement **d√©croissante**"
    else:
        trend = "globalement **stable**"

    vol = float(np.std(series_vals))
    if vol < 0.1 * amplitude:
        vol_text = "peu variable (faible volatilit√©)"
    elif vol < 0.3 * amplitude:
        vol_text = "mod√©r√©ment variable"
    else:
        vol_text = "tr√®s variable (forte volatilit√©)"

    txt = (
        f"- Valeur minimale : **{val_min:.3f}**  \n"
        f"- Valeur maximale : **{val_max:.3f}**  \n"
        f("- Valeur moyenne : **{val_mean:.3f}**  \n")
        f"- Tendance sur la fen√™tre : {trend}  \n"
        f"- Comportement g√©n√©ral : {vol_text}."
    )
    return txt


def describe_proba_distribution(proba_df):
    """
    Analyse la distribution de probas : classe dominante + niveau de confiance.
    proba_df doit √™tre tri√© par probabilit√© d√©croissante.
    """
    top = proba_df.iloc[0]
    cls = int(top["id_class"])
    p_max = float(top["probabilit√©"])

    if len(proba_df) > 1:
        p_second = float(proba_df.iloc[1]["probabilit√©"])
        ratio = p_max / max(p_second, 1e-8)
    else:
        ratio = np.inf

    if ratio > 3:
        conf_text = "Le mod√®le est **tr√®s confiant** (la probabilit√© de la classe 1 est largement sup√©rieure aux autres)."
    elif ratio > 1.5:
        conf_text = "Le mod√®le est **mod√©r√©ment confiant** (la premi√®re classe domine mais les suivantes restent non n√©gligeables)."
    else:
        conf_text = "Le mod√®le est **peu confiant** (plusieurs classes ont des probabilit√©s proches)."

    txt = (
        f"- Classe la plus probable : **{cls}** avec p = **{p_max:.3f}**.  \n"
        f"- {conf_text}"
    )
    return txt


# =====================================================
# 4) CONFIG Streamlit + √©tat
# =====================================================

st.set_page_config(
    page_title="Time Series ‚Äì ML & Deep Learning",
    layout="wide"
)

SEQ_LEN = 10

if "idx_last" not in st.session_state:
    st.session_state.idx_last = SEQ_LEN - 1  # premi√®re position exploitable


# =====================================================
# 5) En-t√™te + contexte (avec description dataset)
# =====================================================

st.title("üïí Classification de signaux capteurs ‚Äì ML vs Deep Learning")

# On charge une premi√®re fois pour donner des chiffres dans le contexte
df_meta, feature_cols_meta = load_data_and_features()
n_samples, n_cols = df_meta.shape
n_features = len(feature_cols_meta)
n_classes = df_meta["id_class"].nunique()

with st.expander("üß¨ Contexte du projet et description des donn√©es", expanded=True):
    st.markdown(f"""
    Ce projet porte sur des **donn√©es de capteurs d'une montre connect√©e**.
    
    - Nombre d'√©chantillons apr√®s pr√©paration : **{n_samples}**  
    - Nombre de features utilis√©es comme entr√©e : **{n_features}**  
      (acc√©l√©rations, gyroscopes, champ magn√©tique, angles + quelques lags temporels)
    - Nombre de classes finales `id_class` (apr√®s regroupement `id_group`) : **{n_classes}**

    Chaque ligne du dataset correspond √† **un instant temporel** :
    `timestamp` + valeurs des capteurs.  
    Les IDs d'intervalle ont √©t√© regroup√©s en **11 classes** plus √©quilibr√©es pour faciliter
    l'apprentissage.

    - **Phase 1 ‚Äì Machine Learning classique :**
      - V√©rification que les donn√©es forment bien une **time series**
      - Pr√©paration / nettoyage / cr√©ation de lags
      - Entra√Ænement de plusieurs mod√®les (R√©gression Logistique, RandomForest, Gradient Boosting‚Ä¶)

    - **Phase 2 ‚Äì Deep Learning :**
      - Mod√®le **MLP** (multilayer perceptron) avec diff√©rents sch√©mas de descente de gradient
      - Mod√®le **LSTM** + RMSprop, qui exploite la dynamique sur une fen√™tre de 10 instants

    L'application ci-dessous permet de tester et comparer en direct :
    - üå≤ un mod√®le **RandomForest** (ML tabulaire, non s√©quentiel)
    - üß† un mod√®le **LSTM + RMSprop** (DL s√©quentiel sur fen√™tre de 10 instants)
    """)


# =====================================================
# 6) Chargement des donn√©es pour l'app
# =====================================================

df, feature_cols = load_data_and_features()
df_seq, X_all, X_all_scaled = prepare_sequential_data(df, feature_cols, seq_len=SEQ_LEN)

num_features = len(feature_cols)
num_classes = len(np.unique(df["id_class"]))

# mapping id_class -> id_group (si PRESENT)
if "id_group" in df_seq.columns:
    map_df = df_seq[["id_class", "id_group"]].drop_duplicates().sort_values("id_class")
    idclass_to_group = dict(zip(map_df["id_class"], map_df["id_group"]))
else:
    idclass_to_group = {}


# =====================================================
# 7) SIDEBAR ‚Äì Navigation temporelle + exploration de classes
# =====================================================

st.sidebar.header("‚öôÔ∏è Navigation temporelle")

max_idx = len(df_seq) - 1
min_idx = SEQ_LEN - 1

# boutons Prev / Next / Random
c_prev, c_next, c_rand = st.sidebar.columns(3)
with c_prev:
    if st.button("‚¨ÖÔ∏è Prev"):
        st.session_state.idx_last = max(min_idx, st.session_state.idx_last - 1)
with c_next:
    if st.button("‚û°Ô∏è Next"):
        st.session_state.idx_last = min(max_idx, st.session_state.idx_last + 1)
with c_rand:
    if st.button("üé≤ Random"):
        st.session_state.idx_last = int(np.random.randint(min_idx, max_idx + 1))

# slider synchronis√©
st.session_state.idx_last = st.sidebar.slider(
    "Index temporel (position dans la s√©rie)",
    min_value=min_idx,
    max_value=max_idx,
    value=st.session_state.idx_last
)

idx_last = st.session_state.idx_last
st.sidebar.write(f"Index s√©lectionn√© : `{idx_last}`")

# exploration par classe
st.sidebar.markdown("---")
st.sidebar.subheader("üîé Explorer par classe")

available_classes = sorted(df_seq["id_class"].unique())
selected_class = st.sidebar.selectbox(
    "Choisir une classe (id_class) :",
    available_classes
)

if st.sidebar.button("Aller √† un exemple de cette classe"):
    indices = df_seq.index[df_seq["id_class"] == selected_class].tolist()
    if indices:
        st.session_state.idx_last = int(np.random.choice(indices))
        idx_last = st.session_state.idx_last
    else:
        st.sidebar.warning("Pas d'√©chantillon pour cette classe dans df_seq.")

# choix de la feature √† tracer
feature_to_plot = st.sidebar.selectbox(
    "Feature √† afficher sur la fen√™tre temporelle :",
    feature_cols,
    index=0
)


# =====================================================
# 8) Affichage info observation
# =====================================================

row = df_seq.iloc[idx_last]
true_class = int(row["id_class"])
true_group = idclass_to_group.get(true_class, "N/A")

st.subheader("üßæ Observation s√©lectionn√©e")

c1, c2 = st.columns(2)

with c1:
    st.write("**Timestamp :**", row["timestamp"])
    st.write("**Classe r√©elle (id_class) :**", true_class)
    st.write("**id_group associ√© :**", true_group)

with c2:
    st.write("**Features au dernier instant (t)**")
    st.dataframe(row[feature_cols].to_frame().T)

st.markdown("---")


# =====================================================
# 9) Contexte temporel (fen√™tre LSTM) + parsing du graphe
# =====================================================

st.subheader("üìà Contexte temporel utilis√© par le LSTM")

start_idx = idx_last - SEQ_LEN + 1
window_df = df_seq.iloc[start_idx:idx_last+1].copy()

st.line_chart(
    window_df.set_index("timestamp")[feature_to_plot],
    height=250
)

st.caption(
    f"√âvolution de `{feature_to_plot}` sur les {SEQ_LEN} instants "
    f"qui pr√©c√®dent t (fen√™tre d'entr√©e du LSTM)."
)

# Analyse textuelle automatique du graphe
series_vals = window_df[feature_to_plot].values
summary_text = describe_time_window(series_vals)

st.markdown("**R√©sum√© automatique de la fen√™tre temporelle :**")
st.info(summary_text)

st.markdown("---")


# =====================================================
# 10) Onglets : RandomForest vs LSTM
# =====================================================

tab_rf, tab_lstm = st.tabs(["üå≤ RandomForest (ML)", "üß† LSTM + RMSprop (DL)"])


# ---------- Onglet RandomForest ----------
with tab_rf:
    st.subheader("üå≤ RandomForest ‚Äì Machine Learning classique")
    st.markdown("""
    **RandomForest** est un ensemble d'arbres de d√©cision.
    Il ne traite pas directement la structure temporelle, mais des **features tabulaires**
    (acc√©l√©rations, gyroscopes, champ magn√©tique, angles + lags cr√©√©s lors de la Phase 1).
    """)

    rf = load_rf_model()
    X_rf = row[feature_cols].values.reshape(1, -1)

    pred_class_rf = int(rf.predict(X_rf)[0])
    proba_rf = rf.predict_proba(X_rf)[0]

    st.write(f"**Classe pr√©dite (id_class) :** `{pred_class_rf}`")
    st.write(f"**Classe r√©elle (id_class) :** `{true_class}`")

    proba_df_rf = pd.DataFrame({
        "id_class": rf.classes_,
        "probabilit√©": proba_rf
    }).sort_values("probabilit√©", ascending=False)

    top_k = 5
    top_df_rf = proba_df_rf.head(top_k)

    c_rf1, c_rf2 = st.columns(2)
    with c_rf1:
        st.write(f"**Top {top_k} classes (RandomForest) :**")
        st.dataframe(top_df_rf)
    with c_rf2:
        st.bar_chart(
            top_df_rf.set_index("id_class")["probabilit√©"],
            height=250
        )

    # Parsing / r√©sum√© du graphe de probas RF
    st.markdown("**Analyse automatique des probabilit√©s (RandomForest) :**")
    st.info(describe_proba_distribution(proba_df_rf))


# ---------- Onglet LSTM ----------
with tab_lstm:
    st.subheader("üß† LSTM + RMSprop ‚Äì Mod√®le s√©quentiel")
    st.markdown("""
    Le **LSTM** re√ßoit en entr√©e une s√©quence de longueur 10  
    (les 10 derniers instants de la s√©rie) et pr√©dit la classe au temps t.
    L'optimiseur utilis√© est **RMSprop**, bien adapt√© aux r√©seaux r√©currents.
    """)

    model_lstm, device = load_lstm_model(
        input_dim=num_features,
        num_classes=num_classes,
        hidden_dim=64,
        num_layers=1
    )

    X_seq = create_seq_from_index(X_all_scaled, idx_last, seq_len=SEQ_LEN)
    X_seq_t = torch.tensor(X_seq, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        logits = model_lstm(X_seq_t)
        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
        pred_class_lstm = int(np.argmax(probs))

    st.write(f"**Classe pr√©dite (id_class) :** `{pred_class_lstm}`")
    st.write(f"**Classe r√©elle (id_class) :** `{true_class}`")

    classes_lstm = sorted(df["id_class"].unique())
    proba_df_lstm = pd.DataFrame({
        "id_class": classes_lstm,
        "probabilit√©": probs
    }).sort_values("probabilit√©", ascending=False)

    top_k = 5
    top_df_lstm = proba_df_lstm.head(top_k)

    c_l1, c_l2 = st.columns(2)
    with c_l1:
        st.write(f"**Top {top_k} classes (LSTM) :**")
        st.dataframe(top_df_lstm)
    with c_l2:
        st.bar_chart(
            top_df_lstm.set_index("id_class")["probabilit√©"],
            height=250
        )

    # Parsing / r√©sum√© du graphe de probas LSTM
    st.markdown("**Analyse automatique des probabilit√©s (LSTM) :**")
    st.info(describe_proba_distribution(proba_df_lstm))


# =====================================================
# 11) Synth√®se RF vs LSTM sur l‚Äôinstant s√©lectionn√©
# =====================================================

st.markdown("---")
st.subheader("üîç Synth√®se de comparaison des mod√®les pour l'instant t s√©lectionn√©")

rf_ok = (pred_class_rf == true_class)
lstm_ok = (pred_class_lstm == true_class)
same_pred = (pred_class_rf == pred_class_lstm)

txt_synth = f"""
- üå≤ **RandomForest** : pr√©diction = `{pred_class_rf}` ‚Üí {"‚úÖ correcte" if rf_ok else "‚ùå incorrecte"}  
- üß† **LSTM + RMSprop** : pr√©diction = `{pred_class_lstm}` ‚Üí {"‚úÖ correcte" if lstm_ok else "‚ùå incorrecte"}  
- üîÅ Les deux mod√®les { "donnent la **m√™me** classe." if same_pred else "donnent des **classes diff√©rentes**." }
"""

st.markdown(txt_synth)


# =====================================================
# 12) R√©sum√© des performances globales
# =====================================================

st.markdown("---")
with st.expander("üìä R√©sum√© des performances globales (sur le jeu de test)", expanded=False):
    st.markdown("""
    **Mod√®les Machine Learning :**
    - üå≤ RandomForest (stratifi√©, avec lags)  
      - Accuracy test ‚âà **0.996**
      - Mod√®le robuste et tr√®s performant sur la majorit√© des classes.

    **Mod√®les Deep Learning :**
    - üß† MLP + SGD + momentum (mini-batch)  
      - Accuracy test ‚âà **0.97**
    - üß† LSTM + RMSprop  
      - Accuracy test ‚âà **0.973**
      - Meilleure prise en compte de la dynamique temporelle (fen√™tre de 10 instants).

    üëâ L'application Web permet de visualiser, pour un instant donn√©,
    comment **ML classique** et **Deep Learning s√©quentiel** se comportent sur les m√™mes donn√©es,
    et d'interpr√©ter leurs d√©cisions via les courbes temporelles et les distributions de probabilit√©s.
    """)

